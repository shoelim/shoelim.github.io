---
#layout: archive
title: "Machine Learning, Dynamical Systems, and All That"
permalink: /DSxML/
author_profile: true
redirect_from:
  - /DSxML
---

## Personal Take

**Deep learning is some kind of optimal control problem** (with the control parameters optimized, for a proper objective, using gradient descent based algorithms and some randomization tricks) **for randomly initialized open dynamical systems** (deep architectures) **interacting with a noisy environment** (large amount of typically noisy data)**, with the hope that the solution found can be applied successfully to new environments** (test data, possibly poor-quality)**.**
<br>


## Selected Papers
### Modern ML x Must-Read:
- [Learning internal representations by error-propagation](https://apps.dtic.mil/sti/pdfs/ADA164453.pdf) (1986)
- [A theoretical framework for back-propagation](http://yann.lecun.com/exdb/publis/pdf/lecun-88.pdf) (1988)
- [Gradient-based learning applied to document recognition](https://ieeexplore.ieee.org/abstract/document/726791) (1998)
- [Random features for large-scale kernel machines](https://papers.nips.cc/paper/2007/hash/013a006f03dbc5392effeb8f18fda755-Abstract.html) (2007)
- [Learning Deep Architectures for AI](https://books.google.se/books?hl=en&lr=&id=cq5ewg7FniMC&oi=fnd&pg=PA1&dq=info:pYyS8T9g_kkJ:scholar.google.com&ots=Kpi8QTmpIy&sig=XfG1389bgdNINpRjGy67OReL9_c&redir_esc=y#v=onepage&q&f=false) (2009)
- [Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a) (2010)
- [ImageNet classification with deep convolutional neural networks](https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html) (2012)
- [Intriguing properties of neural networks](https://arxiv.org/abs/1312.6199) (2013)
- [Explaining and harnessing adversarial examples](https://arxiv.org/abs/1412.6572) (2014)
- [Neural machine translation by jointly learning to align and translate](https://arxiv.org/abs/1409.0473) (2014)
- [Adam: A method for stochastic optimization](https://arxiv.org/abs/1412.6980) (2014)
- [Dropout: A simple way to prevent neural networks from
overfitting](https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer,) (2014)
- [Deep residual learning for image recognition](https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) (2016)
- [Generative Adversarial Nets](https://proceedings.neurips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html) (2014)
- [Batch normalization: accelerating deep network training by reducing internal covariate shift](https://arxiv.org/abs/1502.03167) (2015)
- [Train faster, generalize better: Stability of stochastic gradient descent](https://arxiv.org/pdf/1509.01240.pdf) (2016) 
- [Attention is all you need](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html) (2017)
- [On large-batch training for deep learning: generalization gap and sharp minima](https://arxiv.org/abs/1609.04836) (2017)
- [Towards deep learning models resistant to adversarial attacks](https://arxiv.org/abs/1706.06083) (2017)
- [Train longer, generalize better: closing the generalization gap in large batch training of neural
networks](https://arxiv.org/pdf/1705.08741.pdf) (2018)
- [Super-convergence: very fast training of neural networks using large learning rates](https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11006/1100612/Super-convergence--very-fast-training-of-neural-networks-using/10.1117/12.2520589.full?SSO=1) (2019)
- [Reconciling modern machine-learning practice and the classical bias–variance trade-off](https://www.pnas.org/content/116/32/15849.short) (2019)
- [Language models are few-shot learners](https://arxiv.org/abs/2005.14165) (2020)
- [Hopfield networks is all you need](https://arxiv.org/abs/2008.02217) (2020)
- [Understanding deep learning requires rethinking generalization](https://dl.acm.org/doi/abs/10.1145/3446776) (2017, 2021)


### Sequence Modeling:
- [Unitary Evolution Recurrent Neural Networks](https://arxiv.org/abs/1511.06464) (2016)
- [An empirical evaluation of generic convolutional and recurrent networks for sequence modeling](https://arxiv.org/abs/1803.01271) (2018)
- [Legendre memory units: Continuous-time representation in recurrent neural networks](https://papers.nips.cc/paper/2019/hash/952285b9b7e7a1be5aa7849f32ffff05-Abstract.html) (2019)
- [Do RNN and LSTM have Long Memory?](https://arxiv.org/pdf/2006.03860.pdf) (2020)
- [Long Range Arena: A Benchmark for Efficient Transformers](https://arxiv.org/abs/2011.04006) (2020)
- [Lipschitz Recurrent Neural Networks](https://arxiv.org/abs/2006.12070) (2021)
- [UnICORNN: A recurrent model for learning very long time dependencies](https://arxiv.org/pdf/2103.05487v2.pdf) (2021)
- [Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers](https://paperswithcode.com/paper/combining-recurrent-convolutional-and) (2021)
- [Efficiently Modeling Long Sequences with Structured State Spaces](https://openreview.net/forum?id=uYLFoz1vlAC) (2022)
- [Long Expressive Memory for Sequence Modeling](https://arxiv.org/pdf/2110.04744v2.pdf) (2022)
- [A Neural Programming Language for the Reservoir Computer](https://arxiv.org/pdf/2203.05032.pdf) (2022) 
- [Efficient Transformers: A Survey](https://arxiv.org/pdf/2009.06732.pdf) (2022)
- [Block-Recurrent Transformers](https://arxiv.org/pdf/2203.07852.pdf) (2022)
- [Transformer with Fourier Integral Attentions](https://arxiv.org/abs/2206.00206) (2022)
- [Deep Learning for Time Series Forecasting: Tutorial and Literature Survey](https://arxiv.org/pdf/2004.10240.pdf) (2022)

### Neural Differential Equations and All That:
- [FractalNet: Ultra-deep neural networks without residuals](https://arxiv.org/abs/1605.07648) (2016)
- [A proposal on machine learning via dynamical systems](https://link.springer.com/article/10.1007/s40304-017-0103-z) (2017)
- [Stable architectures for deep neural networks](https://iopscience.iop.org/article/10.1088/1361-6420/aa9a90/meta?casa_token=2bPH9NF1atgAAAAA:s1zabUy4XIbdKQ-2y-q6oJDHE2Zmq3ZdtNFC1cmYdXGfEtnrs1UnATuPRlpm8R1Vg3dmNxk) (2017)
- [The reversible residual network: Backpropagation without storing activations](https://proceedings.neurips.cc/paper/2017/hash/f9be311e65d81a9ad8150a60844bb94c-Abstract.html) (2017)
- [Mean field residual networks: On the edge of chaos](https://proceedings.neurips.cc/paper/2017/hash/81c650caac28cdefce4de5ddc18befa0-Abstract.html) (2017)
- [Beyond finite layer neural networks: Bridging deep architectures and numerical differential equations](http://proceedings.mlr.press/v80/lu18d.html) (2018)
- [Neural Ordinary Differential Equations](https://arxiv.org/abs/1806.07366) (2018)
- [Dynamical isometry and a mean field theory of RNNs: Gating enables signal propagation in recurrent neural networks](http://proceedings.mlr.press/v80/chen18i/chen18i.pdf) (2018)
- [ODE-inspired network design for single image super-resolution](https://openaccess.thecvf.com/content_CVPR_2019/html/He_ODE-Inspired_Network_Design_for_Single_Image_Super-Resolution_CVPR_2019_paper.html) (2019)
- [Invertible Residual Networks](https://proceedings.mlr.press/v97/behrmann19a.html) (2019)
- [Learning differential equations that are easy to solve](https://arxiv.org/pdf/2007.04504.pdf) (2020)
- [Score-based generative modeling through stochastic differential equations](https://arxiv.org/abs/2011.13456) (2020)
- [Continuous-in-Depth Neural Networks](https://arxiv.org/abs/2008.02389) (2020)
- [Optimizing neural networks via Koopman operator theory](https://proceedings.neurips.cc/paper/2020/hash/169806bb68ccbf5e6f96ddc60c40a044-Abstract.html) (2020)
- [Momentum Residual Neural Networks](http://proceedings.mlr.press/v139/sander21a/sander21a.pdf) (2021)
- [Learning strange attractors with reservoir systems](https://arxiv.org/abs/2108.05024) (2021)
- [MALI: A memory efficient and reverse accurate integrator for Neural ODEs](https://arxiv.org/abs/2102.04668) (2021)
- [On Neural Differential Equations](https://arxiv.org/abs/2202.02435) (2022)
- [LyaNet: A Lyapunov Framework for Training Neural ODEs](https://arxiv.org/pdf/2202.02526.pdf) (2022)
- [HyperMixer: An MLP-based Green AI Alternative to Transformers](https://arxiv.org/pdf/2203.03691.pdf) (2022)

### Understanding Modern ML + DS:
- [Neural Tangent Kernel: Convergence and generalization in neural networks](https://proceedings.neurips.cc/paper/2018/hash/5a4be1fa34e62bb8a6ec6b91d2462f5a-Abstract.html) (2018)
- [A mean-field optimal control formulation of deep learning](https://arxiv.org/abs/1807.01083) (2018)
- [Wide neural networks of any depth evolve as linear models under gradient descent](https://proceedings.neurips.cc/paper/2019/hash/0d1a9651497a38d8b1c3871c84528bd4-Abstract.html) (2019)
- [Implicit regularization of discrete gradient dynamics in linear neural networks](https://proceedings.neurips.cc/paper/2019/hash/f39ae9ff3a81f499230c4126e01f421b-Abstract.html) (2019)
- [Stochastic Modified Equations and Dynamics of Stochastic
Gradient Algorithms I: Mathematical Foundations](https://www.jmlr.org/papers/volume20/17-526/17-526.pdf) (2019)
- [Continuous-time models for stochastic optimization algorithms](https://proceedings.neurips.cc/paper/2019/hash/9cd78264cf2cd821ba651485c111a29a-Abstract.html) (2019)
- [Finite depth and width corrections to the Neural Tangent Kernel](https://arxiv.org/abs/1909.05989) (2019)
- [High-dimensional dynamics of generalization error in neural networks](https://www.sciencedirect.com/science/article/pii/S0893608020303117) (2020)
- [Stochasticity of deterministic gradient descent: Large learning rate for multiscale objective function](https://proceedings.neurips.cc//paper/2020/file/1b9a80606d74d3da6db2f1274557e644-Paper.pdf) (2020)
- [Modeling from Features: a Mean-field Framework for Over-parameterized Deep Neural Networks](https://proceedings.mlr.press/v134/fang21a.html) (2021)
- [The heavy-tail phenomenon in SGD](http://proceedings.mlr.press/v139/gurbuzbalaban21a.html) (2021)
- [Gradient descent on neural networks typically occurs at the edge of stability](https://arxiv.org/abs/2103.00065) (2021)
- [SGD in the large: Average-case analysis, asymptotics, and stepsize criticality](https://proceedings.mlr.press/v134/paquette21a.html) (2021)
- [Scaling properties of deep residual networks](https://arxiv.org/abs/2105.12245) (2021)
- [The future is log-Gaussian: ResNets and their infinite-depth-and-width limit at initialization](https://arxiv.org/pdf/2106.04013.pdf) (2021)
- [The high-dimensional asymptotics of first order methods with random data](https://arxiv.org/abs/2112.07572) (2021)
- [Interpolation and approximation via Momentum ResNets and Neural ODEs](https://arxiv.org/pdf/2110.08761.pdf) (2021)
- [Phase diagram of SGD in high-dimensional two-layer neural networks](https://arxiv.org/abs/2202.00293) (2022)
- [Continuous-time stochastic gradient descent for optimizing over the stationary distribution of stochastic differential equations](https://arxiv.org/abs/2202.06637) (2022)
- [Anti-Oversmoothing in Deep Vision Transformers via the Fourier Domain Analysis: From Theory to Practice](https://arxiv.org/abs/2203.05962) (2022)

### Using ML to Study DS:
- [PDE-Net: Learning PDEs from data](http://proceedings.mlr.press/v80/long18a.html?ref=https://githubhelp.com) (2017)
- [Universal Differential Equations for Scientific Machine Learning](https://arxiv.org/abs/2001.04385) (2020)
- [Bridging physics-based and data-driven modeling for learning dynamical systems](https://proceedings.mlr.press/v144/wang21a.html) (2021)
- [An end-to-end deep learning approach for extracting stochastic dynamical systems with α-stable Levy noise](https://arxiv.org/pdf/2201.13114.pdf) (2022)
- [Scientific Machine Learning through Physics-Informed Neural Networks: Where we are and What's next](https://arxiv.org/abs/2201.05624) (2022)
- [When Physics Meets Machine Learning: A Survey of Physics-Informed Machine Learning](https://arxiv.org/abs/2203.16797) (2022)
- Physics-informed neural networks, data-driven discovery of complex systems using neural networks, solving/simulating differential equations using neural networks, integrating ML with physics-based modeling, etc. (too many to list here)

### Model Robustness:
- [Mixup: Beyond Empirical Risk Minimization](https://arxiv.org/abs/1710.09412) (2017)
- [Adversarial Examples Are Not Bugs, They Are Features](https://arxiv.org/pdf/1905.02175.pdf) (2019)
- [Relating Adversarially Robust Generalization to Flat Minima](https://arxiv.org/abs/2104.04448) (2021)
- [PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures](https://arxiv.org/abs/2112.05135) (2021)
- [RobustBench](https://robustbench.github.io/)

### Generative Modeling:
- [Deep Unsupervised Learning using Nonequilibrium Thermodynamics](https://proceedings.mlr.press/v37/sohl-dickstein15.html) (2015)
- [Pixel Recurrent Neural Networks](https://proceedings.mlr.press/v48/oord16.pdf) (2016)
- [Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239.pdf) (2020)
- [Score-Based Generative Modeling through Stochastic Differential Equations](https://arxiv.org/abs/2011.13456) (2020)
- [Diffusion Models Beat GANs on Image Synthesis](https://proceedings.neurips.cc/paper/2021/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html) (2021)
- [Deep Generative Learning via Schrödinger Bridge](https://proceedings.mlr.press/v139/wang21l.html) (2021)
- [Deep Generative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models](https://arxiv.org/abs/2103.04922) (2021)
- [Video Diffusion Models](https://arxiv.org/pdf/2204.03458.pdf) (2022)
- [Flexible Diffusion Modeling of Long Videos](https://arxiv.org/pdf/2205.11495.pdf) (2022)
- [A Continuous Time Framework for Discrete Denoising Models](https://arxiv.org/abs/2205.14987) (2022)

### Complex Networks x Dynamical Systems:
- [Collective dynamics of ‘small-world’ networks](https://www.nature.com/articles/30918) (1998)
- [Emergence of Scaling in Random Networks](https://www.science.org/doi/10.1126/science.286.5439.509) (1999)
- [Exploring complex networks](https://www.nature.com/articles/35065725) (2001)
- [Complex networks: Structure and dynamics](https://www.sciencedirect.com/science/article/pii/S037015730500462X?casa_token=fdsB4rSD7yQAAAAA:2Nm__5s4abTVq9oFlbnc7BNdGcllaVXEP0LK_klJocg7TZaSCH8-Lv0N8wiVneCfHxQDICVjxYdd) (2005)
- [Synchronization in complex networks](https://www.sciencedirect.com/science/article/abs/pii/S0370157308003384) (2008)
- [Recurrence networks—a novel paradigm for nonlinear time series analysis](https://iopscience.iop.org/article/10.1088/1367-2630/12/3/033025/meta) (2010)
- [Deep Reservoir Computing](https://link.springer.com/chapter/10.1007/978-981-13-1687-6_4) (2021)

### Other Noteworthy Papers:
- [The Loss Surfaces of Multilayer Networks](https://arxiv.org/abs/1412.0233) (2014)
- [Visualizing the Loss Landscape of Neural Nets](https://arxiv.org/abs/1712.09913) (2017)
- [Probabilistic supervised learning](https://arxiv.org/pdf/1801.00753.pdf) (2019)
- [On Learning Rates and Schrödinger Operators](https://arxiv.org/abs/2004.06977) (2020)
- [Fourier Neural Operator for Parametric Partial Differential Equations](https://arxiv.org/abs/2010.08895) (2020)
- [Neural Operator: Learning Maps Between Function Spaces](https://arxiv.org/abs/2108.08481) (2021)
- [How Data Augmentation affects Optimization for Linear Regression](https://arxiv.org/pdf/2010.11171.pdf) (2021)
- [Fractal Structure and Generalization Properties of Stochastic Optimization Algorithms](https://proceedings.neurips.cc/paper/2021/file/9bdb8b1faffa4b3d41779bb495d79fb9-Paper.pdf) (2021)
- [Gradient Descent on Infinitely Wide Neural Networks: Global Convergence and Generalization](https://arxiv.org/pdf/2110.08084.pdf) (2021)
- [MLP-Mixer: An all-MLP Architecture for Vision](https://arxiv.org/pdf/2105.01601.pdf) (2021)
- [The curse of overparametrization in adversarial training: Precise analysis of robust generalization for random features regression](https://arxiv.org/pdf/2201.05149.pdf) (2022)
- [Improving generalization via uncertainty driven perturbations](https://arxiv.org/abs/2202.05737) (2022)
- [Benign Overfitting without Linearity: Neural Network Classifiers Trained by Gradient Descent for Noisy Linear Data](https://arxiv.org/abs/2202.05928) (2022)
- [Learning from Randomly Initialized Neural Network Features](https://arxiv.org/abs/2202.06438) (2022)
- [How to Learn when Data Reacts to Your Model: Performative Gradient Descent](https://arxiv.org/pdf/2102.07698.pdf) (2022)
- [Kernel Methods and Multi-layer Perceptrons Learn Linear Models in High Dimensions](https://arxiv.org/pdf/2201.08082.pdf) (2022)
- [Anticorrelated Noise Injection for Improved Generalization](https://arxiv.org/abs/2202.02831) (2022)
- [How Do Vision Transformers Work?](https://arxiv.org/abs/2202.06709) (2022)
- [Patches are all you need?](https://openreview.net/pdf?id=TVHS5Y4dNvM) (2022)
- [Learning by Directional Gradient Descent](https://openreview.net/pdf?id=5i7lJLuhTm) (2022)
- [Gradients without Backpropagation](https://arxiv.org/pdf/2202.08587.pdf) (2022)
- [Continuous-Time Meta-Learning with Forward Mode Differentiation](https://arxiv.org/abs/2203.01443) (2022)
- [General Cyclical Training of Neural Networks](https://arxiv.org/pdf/2202.08835.pdf) (2022)
- [Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer](https://proceedings.neurips.cc/paper/2021/file/8df7c2e3c3c3be098ef7b382bd2c37ba-Paper.pdf) (2022)
- [Understanding Gradient Descent on Edge of Stability in Deep Learning](https://arxiv.org/pdf/2205.09745.pdf) (2022)
- [Reconstructing Training Data from Trained Neural Networks](https://arxiv.org/pdf/2206.07758v1.pdf) (2022)

## Related Review Papers/Monographs/Textbooks/Lecture Notes
- [Solving Ordinary Differential Equations I](https://link.springer.com/book/10.1007/978-3-540-78862-1) (1993)
- [Dynamical Systems and Numerical Analysis](https://books.google.se/books?hl=en&lr=&id=ymoQA8s5pNIC&oi=fnd&pg=PR11&dq=dynamical+systems+and+numerical+analysis&ots=TYk2JZiNVG&sig=0mCqPchp17JHceSdTerWUMjjAhE&redir_esc=y#v=onepage&q=dynamical%20systems%20and%20numerical%20analysis&f=false) (1996)
- [Information Theory, Inference, and Learning Algorithms](http://www.inference.org.uk/mackay/itila/book.html) (2003)
- [All of Statistics](https://link.springer.com/book/10.1007/978-0-387-21736-9) (2004)
- [Information, Physics, and Computation](https://web.stanford.edu/~montanar/RESEARCH/book.html) (2009) 
- [The Elements of Statistical Learning](https://link.springer.com/book/10.1007/978-3-319-31089-3) (2009)
- [Probability and Stochastics](https://link.springer.com/book/10.1007/978-0-387-87859-1) (2011)
- [Understanding Machine Learning: From Theory to Algorithms](https://www.cs.huji.ac.il/w~shais/UnderstandingMachineLearning/) (2014)
- [Deep Learning](https://www.deeplearningbook.org/) (2016)
- [Brownian Motion, Martingales, and Stochastic Calculus](https://link.springer.com/book/10.1007/978-3-319-31089-3) (2016)
- [High-Dimensional Probability: An Introduction with Applications in Data Science](https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.html) (2018)
- [High-Dimensional Statistics: A Non-Asymptotic Viewpoint](https://www.cambridge.org/core/books/highdimensional-statistics/8A91ECEEC38F46DAB53E9FF8757C7A4E) (2019)
- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) (2019)
- [Statistical mechanics of deep learning](https://www.annualreviews.org/doi/abs/10.1146/annurev-conmatphys-031119-050745) (2019)
- [Machine learning and the physical sciences](https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.91.045002) (2019)
- [A Course in Machine Learning by Hal Daumé III](http://ciml.info/) 
- [Dive Into Deep Learning](https://d2l.ai/)
- [Deep Learning (NYU), Spring 2020](https://atcold.github.io/pytorch-Deep-Learning/)
- [Deep Learning Theory Review: An Optimal Control and Dynamical Systems Perspective](https://arxiv.org/abs/1908.10920) (2019)
- [Machine Learning, Dynamical Systems and Control](http://databookuw.com/) (2020)
- [Towards a Mathematical Understanding of Neural Network-Based Machine Learning: what we know and what we don't](https://arxiv.org/abs/2009.10713) (2020)
- [Theoretical issues in deep networks](https://www.pnas.org/content/117/48/30039) (2020)
- [Mathematics for Machine Learning](https://mml-book.github.io/) (2020)
- [An Introduction to the Numerical Simulation of Stochastic Differential Equations](https://www.amazon.com/Introduction-Numerical-Simulation-Stochastic-Differential/dp/1611976421) (2021)
- [Patterns, predictions, and actions: A story about machine learning](https://mlstory.org/) (2021)
- [Foundations of Deep Learning (Maryland), Fall 2021](http://www.cs.umd.edu/class/fall2021/cmsc828W/)
- [Dynamical Systems and Machine Learning](https://www.math.pku.edu.cn/amel/docs/20200719122925684287.pdf) (2021)
- [The Principles of Deep Learning Theory](https://arxiv.org/abs/2106.10165) (2021)
- [Deep learning: A statistical viewpoint](https://www.cambridge.org/core/journals/acta-numerica/article/deep-learning-a-statistical-viewpoint/7BCB89D860CEDDD5726088FAD64F2A5A) (2021)
- [Fit without fear: Remarkable mathematical phenomena of deep learning through the prism of interpolation](https://www.cambridge.org/core/journals/acta-numerica/article/fit-without-fear-remarkable-mathematical-phenomena-of-deep-learning-through-the-prism-of-interpolation/DBAC769EB7F4DBA5C4720932C2826014) (2021)
- [Rough Path Theory (ETH), Spring 2021](https://metaphor.ethz.ch/x/2021/fs/401-4611-21L/#recordings)
- [Nonlinear Dynamics (Georgia Tech), Spring 2022](https://chaosbook.org/course1/about.html) 
- [Probabilistic Machine Learning](https://probml.github.io/pml-book/) (2022)
- [Parallel Computing and Scientific Machine Learning (MIT)](https://mitmath.github.io/18337/) 
- [Ergodic Theory (Lecture Notes)](https://www.ma.imperial.ac.uk/~mrasmuss/ergodictheory/ErgodicTheoryNotes.pdf)
- [Modern applications of machine learning in quantum sciences](https://arxiv.org/abs/2204.04198) (2022) 
- [A high bias low-variance introduction to Machine Learning for physicists](https://physics.bu.edu/~pankajm/MLnotebooks.html) 

## Softwares/Libraries
- [TensorFlow](https://www.tensorflow.org/)
- [PyTorch](https://pytorch.org/)
- [JAX](https://jax.readthedocs.io/en/latest/)
- [SciML Scientific Machine Learning Software](https://sciml.ai/roadmap/)


## Others (related articles/blogposts/tutorials and random cool stuffs)
- [Papers with Codes](https://paperswithcode.com/)
- [Off the Convex Path](https://www.offconvex.org/)
- [How to Differentiate with a Computer](https://www.ams.org/publicoutreach/feature-column/fc-2017-12)
- [Almost Sure](https://almostsuremath.com/)
- [Fabrice Baudoin](https://fabricebaudoin.wordpress.com/)
- [Terrence Tao](https://terrytao.wordpress.com/)
- [Deep Learning: Our Miraculous Year 1990-1991](https://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html)
- [Universality for mathematical and physical systems](https://arxiv.org/abs/math-ph/0603038#) (2006)
- [One model to learn them all](https://arxiv.org/abs/1706.05137) (2017)
- [Winner's curse? On pace, progress, and empirical rigor](https://openreview.net/forum?id=rJWF0Fywf) (2018)
- [The science of deep learning](https://www.pnas.org/content/117/48/30029) (2020)
- [The Dawning of a New Era in Applied Mathematics](https://www.ams.org/journals/notices/202104/rnoti-p565.pdf) (2021)
- [AI Accelerators](https://medium.com/@adi.fu7/ai-accelerators-part-i-intro-822c2cdb4ca4)
- [AI Summer](https://theaisummer.com/) 
- [Full Stack Deep Learning](https://fullstackdeeplearning.com/spring2021/)
- [Machine Learning Systems Design](https://stanford-cs329s.github.io/syllabus.html)
- [Physics-based Deep Learning](https://physicsbaseddeeplearning.org/intro.html)
- [A Recipe for Training Neural Networks](http://karpathy.github.io/2019/04/25/recipe/)
- [Implicit Bias in Some Machine Learning Problems](https://hanin.princeton.edu/implicitbiastalk.pdf)
- [Sebastian Raschka's Resources](https://sebastianraschka.com/resources/)
- [ML-University](https://github.com/d0r1h/ML-University#machine-learning)
- [Differential Programming with JAX](https://ericmjl.github.io/dl-workshop/)
- [Teach Yourself Computer Science](https://teachyourselfcs.com/)
- [Software Carpentry](https://software-carpentry.org/lessons/index.html)
- [Software 2.0](https://karpathy.medium.com/software-2-0-a64152b37c35) (see also [here](https://www.youtube.com/watch?v=y57wwucbXR8))
- [Green AI](https://arxiv.org/abs/1907.10597)
- [Hugging Face](https://huggingface.co/)
- [Tesla AI](https://www.tesla.com/AI)
- [Web3 University](https://www.web3.university/)
- [Xanadu](https://www.xanadu.ai/) 
